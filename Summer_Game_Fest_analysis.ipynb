{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45646bb1",
   "metadata": {},
   "source": [
    "# The analysis of Twitter users' reception of Summer Game Fest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89a0839",
   "metadata": {},
   "source": [
    "## Downloading tweets with snscrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01fbc2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping the number of comments, retweets and favourites requires the developer version of snscrape.\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dedcf7",
   "metadata": {},
   "source": [
    "Downloaded tweets need to: include the hashtag \"SummerGameFest\", be in the english language and have some form o reaction from the users. I've decided to only use tweets coming from verified users - that'll help me avoid comments from the press or the organizers of the event. This tutorial on github has helped me with the syntax of Twitter filters: https://github.com/igorbrigadir/twitter-advanced-search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ce2c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "\n",
    "max_tweets = 20000\n",
    "\n",
    "for count, tweet in enumerate(sntwitter.TwitterSearchScraper('#SummerGameFest lang:en since:2022-06-09 until:2022-06-14 filter:has_engagement -filter:verified').get_items()):\n",
    "    if count > max_tweets:\n",
    "        break\n",
    "    tweets.append([tweet.date, tweet.user.username, tweet.content, tweet.replyCount, tweet.retweetCount, tweet.likeCount])\n",
    "        \n",
    "twitter_df = pd.DataFrame(tweets, columns = [\"Date\", \"User\", \"Text\", \"Replies\", \"Retweets\", \"Likes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cf3eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea37176",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8fca82",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98763155",
   "metadata": {},
   "outputs": [],
   "source": [
    "#twitter_df.to_csv(\"game_fest.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae74281",
   "metadata": {},
   "source": [
    "## roBERTa model for sentiment analysis of tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219b5cf7",
   "metadata": {},
   "source": [
    "For sentiment analysis, I employed the model from: https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5a7b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from scipy.special import softmax\n",
    "\n",
    "labels = [\"negative\", \"neutral\", \"positive\"]\n",
    "\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    " \n",
    " \n",
    "    for t in text.split(\" \"):\n",
    "        if t.startswith('@') and len(t) > 1:\n",
    "            t = '@user' \n",
    "        elif t.startswith('http'):\n",
    "            t = 'http'\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "\n",
    "roberta = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19263fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_tokenizer = AutoTokenizer.from_pretrained(roberta)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def get_sentiment(tweet):\n",
    "    \"\"\"\n",
    "    Input: Tweets from Twitter\n",
    "    Usage: get_sentiment() classifies the tweet into one of three labels\n",
    "    Output: Appropriate label of the tweet\n",
    "    \"\"\"\n",
    "    encoded_tweet = auto_tokenizer(preprocess(tweet), return_tensors='pt')\n",
    "\n",
    "    output = model(**encoded_tweet)\n",
    "\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    \n",
    "    ranking = np.argsort(scores)\n",
    "    ranking = ranking[::-1]\n",
    "    \n",
    "    return labels[ranking[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79b540d",
   "metadata": {},
   "source": [
    "## Cleaning and processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681c4009",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f691893d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#game_fest = pd.read_csv(\"game_fest.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e86a8a6",
   "metadata": {},
   "source": [
    "Creating a column with the sentiment of tweets. This process takes some time to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7a0b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#game_fest[\"Sentiment\"] = [get_sentiment(tweet) for tweet in game_fest[\"Text\"]]\n",
    "#game_fest.to_csv(\"game_fest.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816c77a3",
   "metadata": {},
   "source": [
    "In orderd to keep the research family friendly, I've deleted profanities from the text (it is optional, though). This process can also take quite some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fdc0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from better_profanity import profanity\n",
    "\n",
    "#game_fest[\"Text\"] = game_fest[\"Text\"].map(lambda x: profanity.censor(x))\n",
    "\n",
    "#game_fest.to_csv(\"game_fest_censor.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cfd9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_fest = pd.read_csv(\"game_fest_censor.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661cd781",
   "metadata": {},
   "source": [
    "After deleting duplicates, I am left with 14683 tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e2d438",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_fest = game_fest.drop_duplicates(subset=[\"Text\"])\n",
    "game_fest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca55f88f",
   "metadata": {},
   "source": [
    "For tokenization I've chosen the TweetTokenizer from nltk.tokenize module, as it is the best suited tool for tokenizing tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb4c4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tokenizer = TweetTokenizer(strip_handles=True)\n",
    "\n",
    "game_fest[\"Text\"] = game_fest[\"Text\"].apply(lambda x: tokenizer.tokenize(x))\n",
    "game_fest[\"Text\"] = [[token.lower() for token in tokens] for tokens in game_fest[\"Text\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7aa83a5",
   "metadata": {},
   "source": [
    "All the links have been deleted with regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9123e5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "game_fest[\"Text\"] = [[re.sub(r'http\\S+', '', token) for token in tokens] for tokens in game_fest[\"Text\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680970c1",
   "metadata": {},
   "source": [
    "Deleting stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41197ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "game_fest[\"Text\"] = [[token for token in tokens if token not in stop_words] for tokens in game_fest[\"Text\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20216939",
   "metadata": {},
   "source": [
    "Lemmatization of the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07340e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "game_fest[\"Text\"] = [[lemmatizer.lemmatize(token) for token in tokens] for tokens in game_fest[\"Text\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c769ee",
   "metadata": {},
   "source": [
    "Getting rid of punctuation and numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f97f2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "game_fest[\"Text\"] = [[token for token in tokens if token not in string.punctuation] for tokens in game_fest[\"Text\"]]\n",
    "game_fest[\"Text\"] = [[re.sub('[,\\\\.!?]', '', token) for token in tokens] for tokens in game_fest[\"Text\"]]\n",
    "game_fest[\"Text\"] = [[token for token in tokens if not token.isnumeric()] for tokens in game_fest[\"Text\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316b2674",
   "metadata": {},
   "source": [
    "Deleting tokens shorter than 1 characters will help avoid emoticons, which have no purpose at this stage of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b64c722",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_fest[\"Text\"] = [[token for token in tokens if len(token)>1] for tokens in game_fest[\"Text\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedc540e",
   "metadata": {},
   "source": [
    "Deleting words common in hashtags related to the event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fb9add",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_words = ['summergamefest', 'pcgamingshow', 'xboxbethesda', 'dayofthedevs']\n",
    "game_fest[\"Text\"] = [[token for token in tokens if not any(word in token for word in delete_words)] for tokens in game_fest[\"Text\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7bb124",
   "metadata": {},
   "source": [
    "N-grams will help in interpretation of topics from LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88341558",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "from gensim.models import phrases\n",
    "\n",
    "bigram = Phrases(game_fest[\"Text\"], min_count=5, threshold=100)\n",
    "trigram = Phrases(bigram[game_fest[\"Text\"]], threshold=100)\n",
    "\n",
    "bigram_mod = phrases.Phraser(bigram)\n",
    "trigram_mod = phrases.Phraser(trigram)\n",
    "\n",
    "game_fest[\"Text\"] = [bigram_mod[tokens] for tokens in game_fest[\"Text\"]]\n",
    "game_fest[\"Text\"] = [trigram_mod[tokens] for tokens in game_fest[\"Text\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6834091",
   "metadata": {},
   "source": [
    "I'll decide the minimum length of tweet based on the countplot of tweets' lenghts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaa3063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "len_tweets = [len(tweet) for tweet in game_fest[\"Text\"]]\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "plt.figure(figsize = (10,5))\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "sns.countplot(x = len_tweets)\n",
    "plt.xlabel('Liczba słów', fontsize = 16)\n",
    "plt.ylabel('Liczba tweetów', fontsize = 16)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig('tweets_lengths.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1407f38f",
   "metadata": {},
   "source": [
    "Tweets with at least 4 words will be left in the dataset. This gives me 11855 tweets to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d45ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_fest = game_fest[game_fest['Text'].map(len)>3]\n",
    "game_fest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fc006d",
   "metadata": {},
   "source": [
    "## Timeline tweets' sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183c3971",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_fest['Dates'] = pd.to_datetime(game_fest['Date']).dt.date\n",
    "game_fest['Hour'] = pd.to_datetime(game_fest['Date']).dt.floor('H').dt.time\n",
    "\n",
    "game_fest['Dates'] = game_fest['Dates'].map(lambda x: str(x))\n",
    "game_fest['Hour'] = game_fest['Hour'].map(lambda x: str(x))\n",
    "game_fest['Hour'] = game_fest['Hour'].map(lambda x: x[:5])\n",
    "\n",
    "game_fest['Date'] = game_fest['Dates']+' '+game_fest['Hour']\n",
    "\n",
    "ts_df = game_fest.groupby(by=[\"Date\"])[\"Sentiment\"].value_counts().unstack()\n",
    "ts_df=ts_df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec98978",
   "metadata": {},
   "source": [
    "Publication time is in the UTC time zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6a1216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "plt.figure(figsize = (10,6))\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "sns.lineplot(x='Date', y='positive', color='blue', data = ts_df)\n",
    "sns.lineplot(x='Date', y='neutral', color='green', data = ts_df)\n",
    "ts_plot=sns.lineplot(x='Date', y='negative', color='red', data = ts_df)\n",
    "ts_plot.set_xlabel(\"\")\n",
    "ts_plot.set_ylabel(\"Liczba tweetów\", fontsize = 16)\n",
    "plt.xticks(fontsize=14, rotation='vertical')\n",
    "plt.yticks(fontsize=14)\n",
    "ts_plot.set(yscale='log')\n",
    "ts_plot.set(ylim=1)\n",
    "\n",
    "plt.axvline(\"2022-06-09 00:00\", color='black', linestyle=\"--\")\n",
    "plt.axvline(\"2022-06-10 00:00\", color='black', linestyle=\"--\")\n",
    "plt.axvline(\"2022-06-11 00:00\", color='black', linestyle=\"--\")\n",
    "plt.axvline(\"2022-06-12 00:00\", color='black', linestyle=\"--\")\n",
    "plt.axvline(\"2022-06-13 00:00\", color='black', linestyle=\"--\")\n",
    "plt.legend(labels=[\"pozytywny\", \"neutralny\", \"negatywny\"], fontsize = 16)\n",
    "\n",
    "for ind, label in enumerate(ts_plot.get_xticklabels()):\n",
    "    if ind % 24 == 0:\n",
    "        label.set_visible(True)\n",
    "    else:\n",
    "        label.set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig('tweets_time.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823e221f",
   "metadata": {},
   "source": [
    "The reception of the event was generally positive. The most popular days were the 9th and 12th of June."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f8aed9",
   "metadata": {},
   "source": [
    "## Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d53b8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_fest_pos=game_fest.loc[game_fest.Sentiment == \"positive\"]\n",
    "game_fest_neg=game_fest.loc[game_fest.Sentiment == \"negative\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16753d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_pos = []\n",
    "\n",
    "long_neg = []\n",
    "\n",
    "for tweet in game_fest_pos['Text']:\n",
    "    long_pos.append(' '.join(tweet))\n",
    "\n",
    "for tweet in game_fest_neg['Text']:\n",
    "    long_neg.append(' '.join(tweet))\n",
    "    \n",
    "print(\"Vocabulary's lenght in postive tweets: \", len(set(long_pos)))\n",
    "\n",
    "print(\"Vocabulary's lenght in negative tweets:: \", len(set(long_neg)))\n",
    "\n",
    "long_pos = ','.join(long_pos)\n",
    "long_neg = ','.join(long_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be1ce9b",
   "metadata": {},
   "source": [
    "### Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff170546",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "wordcloud = WordCloud(width=1600, height=800, background_color=\"white\", max_words=500).generate(long_pos)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.imshow(wordcloud)\n",
    "#plt.savefig('pos_wordcloud.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3903f55",
   "metadata": {},
   "source": [
    "### Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d87b77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width=1600, height=800, background_color=\"black\", max_words=500).generate(long_neg)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.imshow(wordcloud)\n",
    "#plt.savefig('neg_wordcloud.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c6ddbf",
   "metadata": {},
   "source": [
    "## Model LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3a523a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "dictionary = Dictionary(game_fest[\"Text\"])\n",
    "corpus = [dictionary.doc2bow(tweet) for tweet in game_fest[\"Text\"]]\n",
    "id2word=dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013c21ec",
   "metadata": {},
   "source": [
    "### Coherence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72686c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from gensim.models import CoherenceModel\n",
    "import numpy as np\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6c7ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(corpus, dictionary, k, a, b):\n",
    "    \"\"\"\n",
    "    Input: Model's paramethers\n",
    "    Purpose: Function outputs the coherence score for different LDA models.\n",
    "    Output: Coherence score\n",
    "    \"\"\"\n",
    "    lda_model = LdaModel(corpus=corpus,\n",
    "                         id2word=dictionary,\n",
    "                         num_topics=k, \n",
    "                         random_state=100,\n",
    "                         chunksize=len(corpus),\n",
    "                         passes=10,\n",
    "                         alpha=a,\n",
    "                         eta=b)\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=game_fest['Text'], dictionary=id2word, coherence='c_v')\n",
    "    \n",
    "    return coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc84862",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {}\n",
    "grid['Validation_Set'] = {} \n",
    "\n",
    "min_topics = 4\n",
    "max_topics = 22\n",
    "step_size = 2\n",
    "topics_range = range(min_topics, max_topics, step_size)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "num_of_tweets = len(corpus)\n",
    "corpus_sets = [corpus]\n",
    "corpus_title = ['100% Corpus']\n",
    "\n",
    "alpha=['symmetric', 'asymmetric']\n",
    "beta=['symmetric']\n",
    "\n",
    "model_results = {'Validation_Set': [],\n",
    "                 'Topics': [],\n",
    "                 'Alpha': [],\n",
    "                 'Beta': [],\n",
    "                 'Coherence': []\n",
    "                }\n",
    "\n",
    "if 1 == 1:\n",
    "    pbar = tqdm.tqdm(total=len(corpus_sets)*len(topics_range)*len(alpha)*len(beta))\n",
    "    \n",
    "    for i in range(len(corpus_sets)):\n",
    "        for k in topics_range:\n",
    "            for a in alpha:\n",
    "                for b in beta:\n",
    "                    print(i, k, a, b)\n",
    "                    \n",
    "                    cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=id2word,\n",
    "                                                            k=k, a=a, b=b)\n",
    "                    model_results['Validation_Set'].append(corpus_title[i])\n",
    "                    model_results['Topics'].append(k)\n",
    "                    model_results['Alpha'].append(a)\n",
    "                    model_results['Beta'].append(b)\n",
    "                    model_results['Coherence'].append(cv)\n",
    "                    print({k: v[-1] for k, v in model_results.items()})\n",
    "\n",
    "                    pbar.update(1)\n",
    "    pbar.close()\n",
    "    model_results=pd.DataFrame(model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc0d3bd",
   "metadata": {},
   "source": [
    "### Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9035bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics=20\n",
    "passes=10\n",
    "\n",
    "lda_model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=len(corpus),\n",
    "    alpha='asymmetric',\n",
    "    eta='symmetric',\n",
    "    random_state=100,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    per_word_topics=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb14837e",
   "metadata": {},
   "source": [
    "### Topic visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf130ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n",
    "\n",
    "vis=pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n",
    "#pyLDAvis.save_html(vis, 'lda.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bf443c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7f8e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
    "\n",
    "cloud = WordCloud(background_color='white',\n",
    "                  width=1200,\n",
    "                  height=1600,\n",
    "                  max_words=30,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "topics = [lda_model.show_topic(0, 30), lda_model.show_topic(7, 30), lda_model.show_topic(8, 30)]\n",
    "topic_nums = [\"1\", \"6\", \"8\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12,16), sharex=True, sharey=True)\n",
    "\n",
    "for i, (ax, num) in enumerate(zip(axes.flatten(), topic_nums)):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + num, fontdict=dict(size=26))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.imshow(cloud)\n",
    "plt.savefig('topics_lda.png', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tm_env]",
   "language": "python",
   "name": "conda-env-tm_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
